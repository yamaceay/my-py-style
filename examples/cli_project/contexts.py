#!/usr/bin/env python3
"""
Resource Lifecycle Management (Context Managers)

This module implements Rule #5: Context managers for lifecycle.
Heavy resources must be scoped with `with ...:` for explicit construction and cleanup.

Generated by: Go-ish Python Scaffolder
Author: YamaÃ§ Eren Ay

Modification Guidelines:
- Use AbstractContextManager for type safety
- Initialize resources in __enter__, cleanup in __exit__
- Return the managed resource from __enter__ (not self)
- Handle exceptions appropriately in __exit__
- Log lifecycle events when logging is enabled
- Follow Rule #1: static deps in __init__, dynamic inputs in methods

Rule #5 Examples:
  GOOD: Heavy resource management
        with ServiceContext(config) as service:
            result = service.execute(data)  # Automatic cleanup
  
  BAD:  Manual resource management
        service = create_service(config)
        try:
            result = service.execute(data)
        finally:
            service.cleanup()  # Manual cleanup, might be forgotten
"""

from __future__ import annotations
from contextlib import AbstractContextManager
from typing import TYPE_CHECKING, Any
import os
import sys
import logging

if TYPE_CHECKING:
    from .config import AppConfig

# Smart import for core implementations
try:
    from .core import DefaultService, DefaultRepository, DefaultProcessor
except ImportError:
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    from core import DefaultService, DefaultRepository, DefaultProcessor

logger = logging.getLogger(__name__)

class ServiceContext(AbstractContextManager[DefaultService]):
    """
    Context manager for service lifecycle (Rule #5).
    
    Manages the complete lifecycle of service and its dependencies:
    - Repository creation and connection
    - Processor initialization 
    - Service composition
    - Automatic cleanup on exit
    
    Static dependencies go in __init__ (Rule #1).
    """
    
    def __init__(self, config: "AppConfig"):
        """Static dependency: config is passed once and never changes (Rule #1)."""
        self.config = config
        self.service: DefaultService | None = None
        self.repository: DefaultRepository | None = None
        self.processor: DefaultProcessor | None = None
    
    def __enter__(self) -> DefaultService:
        """Initialize all resources - explicit construction (Rule #5)."""
        logger.info('initializing_service_context', extra={'app_name': self.config.app_name})
        
        # Initialize components with proper dependency injection (Rule #1)
        # Static deps from config, no re-detection (Rule #3 & #4)
        self.repository = DefaultRepository(storage_config="memory")
        self.processor = DefaultProcessor(
            name=self.config.processor_name,  # Use computed property
            processing_mode="default"
        )
        
        # Compose service with injected dependencies
        self.service = DefaultService(
            repository=self.repository,
            processor=self.processor,
            service_name=self.config.app_name
        )
        
        logger.info('service_context_ready', extra={'service_name': self.config.app_name, 'verbose': self.config.verbose})
        return self.service
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
        """Explicit cleanup - guaranteed to run (Rule #5)."""
        logger.info('releasing_service_resources', extra={'had_exception': exc_type is not None})
        
        # Clean up in reverse order of initialization
        if self.service:
            # Service might have cleanup logic
            pass
            
        if self.repository:
            # Repository might need to close connections, flush data, etc.
            pass
            
        if self.processor:
            # Processor might need to free GPU memory, close files, etc.
            pass
        
        # Clear references
        self.service = None
        self.repository = None  
        self.processor = None
        
        logger.info('service_resources_released')
        return False  # Don't suppress exceptions

class DataContext(AbstractContextManager["DataContext"]):
    """
    Context manager for data loading and cleanup (Rule #5).
    
    Example of resource management for data sources:
    - File handles
    - Database connections  
    - Network streams
    - Large datasets in memory
    """
    
    def __init__(self, data_source: str, config: "AppConfig"):
        """Static dependencies: data_source and config (Rule #1)."""
        self.data_source = data_source
        self.config = config
        self.data: list[Any] | None = None
        self._file_handle = None
        self._connection = None
    
    def __enter__(self) -> "DataContext":
        """Load data resources - explicit acquisition (Rule #5)."""
        logger.info('loading_data', extra={'source': self.data_source, 'output_dir': str(self.config.output_dir)})
        
        # Example: Load data based on source type
        if self.data_source.startswith("file://"):
            # Would open file handle here
            # self._file_handle = open(self.data_source[7:], 'r')
            self.data = []  # Simulate file data
        elif self.data_source.startswith("db://"):
            # Would open database connection here
            # self._connection = create_connection(self.data_source)
            self.data = []  # Simulate database data
        else:
            # Default in-memory data
            self.data = []
        
        logger.info('data_loaded', extra={'count': len(self.data), 'source_type': self.data_source.split('://')[0] if '://' in self.data_source else 'default'})
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
        """Explicit cleanup - free resources (Rule #5)."""
        logger.info('releasing_data_resources', extra={'had_exception': exc_type is not None})
        
        # Clean up resources in reverse order
        if self._connection:
            # Would close database connection
            # self._connection.close()
            self._connection = None
            
        if self._file_handle:
            # Would close file handle
            # self._file_handle.close()
            self._file_handle = None
        
        # Clear large data from memory
        if self.data:
            self.data.clear()
        self.data = None
        
        logger.info('data_resources_released')
        return False
    
    def get_data(self) -> list[Any]:
        """Get loaded data - only accepts dynamic input: none needed here."""
        if self.data is None:
            raise RuntimeError("Data not initialized - use within context manager")
        return self.data
    
    def add_data(self, item: Any) -> None:
        """Add data item - dynamic input only (Rule #1)."""
        if self.data is None:
            raise RuntimeError("Data not initialized - use within context manager") 
        self.data.append(item)

class ModelContext(AbstractContextManager["ModelContext"]):
    """
    Example context manager for ML model lifecycle (Rule #5).
    
    Demonstrates managing:
    - GPU memory allocation
    - Model loading from disk
    - Tokenizer initialization
    - Automatic cleanup
    """
    
    def __init__(self, model_path: str, device: str, config: "AppConfig"):
        """Static dependencies: model_path, device, config (Rule #1)."""
        self.model_path = model_path
        self.device = device
        self.config = config
        self.model = None
        self.tokenizer = None
    
    def __enter__(self) -> "ModelContext":
        """Load model resources - heavy resource acquisition (Rule #5)."""
        logger.info('loading_model', extra={'model_path': self.model_path, 'device': self.device})
        
        # Simulate model loading (would use actual ML libraries)
        # self.model = torch.load(self.model_path, map_location=self.device)
        # self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = f"model_on_{self.device}"
        self.tokenizer = f"tokenizer_for_{self.model_path}"
        
        logger.info('model_loaded', extra={'device': self.device, 'verbose': self.config.verbose})
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
        """Explicit cleanup - free GPU memory (Rule #5)."""
        logger.info('releasing_model_resources', extra={'device': self.device})
        
        # Free GPU memory and model resources
        if self.model:
            # Would call model.cpu(), del model, torch.cuda.empty_cache(), etc.
            self.model = None
            
        if self.tokenizer:
            # Would clear tokenizer cache if needed
            self.tokenizer = None
        
        logger.info('model_resources_released')
        return False
    
    def predict(self, text: str) -> str:
        """Make prediction - dynamic input only (Rule #1)."""
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("Model not initialized - use within context manager")
        
        # Would do actual prediction here
        return f"prediction_for_{text}_using_{self.model}"

# Example usage patterns (following all rules):
def example_proper_context_usage(config: "AppConfig") -> dict[str, Any]:
    """
    Example showing proper context manager usage following all rules.
    
    Demonstrates:
    - Rule #1: Static deps in function signature, dynamic inputs in methods
    - Rule #2: Early selection, no branching
    - Rule #3 & #4: Single config source, passed down
    - Rule #5: Context managers for heavy resources
    - Rule #6: Structured logging over print
    """
    results = []
    
    # Multiple context managers can be nested or chained
    with ServiceContext(config) as service:
        with DataContext("memory://default", config) as data_ctx:
            
            # Add some test data (dynamic input)
            data_ctx.add_data({"type": "test", "value": 1})
            data_ctx.add_data({"type": "test", "value": 2})
            
            # Process all data
            for item in data_ctx.get_data():
                result = service.execute(item)  # Only dynamic input
                results.append(result)
    
    # Both contexts automatically cleaned up here
    return {"processed_items": len(results), "results": results}

# Anti-patterns (DO NOT DO THIS):
#
# WRONG - Manual resource management (violates Rule #5):
# def bad_manual_cleanup():
#     service = DefaultService(...)
#     try:
#         result = service.execute(data)
#     finally:
#         service.cleanup()  # Might be forgotten!
#
# WRONG - No context manager for heavy resources (violates Rule #5):
# def bad_no_context():
#     model = load_gpu_model()  # GPU memory allocated
#     result = model.predict(text)
#     # GPU memory never freed!
#     return result
